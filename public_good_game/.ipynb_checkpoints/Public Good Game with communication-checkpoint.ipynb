{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import ceil, floor\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import Normal\n",
    "from scipy.stats import rv_discrete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.all_rewards = []\n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.mexs = []\n",
    "        self.log_probs_mexs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def push(self, s, a, lp):\n",
    "        self.states.append(s)        \n",
    "        self.actions.append(a)        \n",
    "        self.log_probs.append(lp)\n",
    "        \n",
    "    def push_comm(self, s, a, lp):\n",
    "        self.states.append(s)        \n",
    "        self.mexs.append(a)        \n",
    "        self.log_probs_mexs.append(lp)\n",
    "        \n",
    "    def save_reward(self, r):\n",
    "        self.rewards.append(r)        \n",
    "        self.all_rewards.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CommNet(nn.Module):\n",
    "    def __init__(self, n_coins, lr=0.01):\n",
    "        super(CommNet, self).__init__()\n",
    "        \n",
    "        self.n_coins = n_coins\n",
    "        self.lr = lr\n",
    "        self.n_actions = 2 # 0 non do soldi, 1 do soldi\n",
    "        \n",
    "        h_size = int(n_coins)\n",
    "        a_size = self.n_actions\n",
    "        self.fc1 = nn.Linear(n_coins+1, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "        \n",
    "        self.opt = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActNet(nn.Module):\n",
    "    def __init__(self, n_players, n_coins, n_actions=2, lr = 0.01):\n",
    "        super(ActNet, self).__init__()\n",
    "        \n",
    "        self.n_players = n_players\n",
    "        self.n_coins = n_coins\n",
    "        self.n_actions = n_actions # 0 non do soldi, 1 do soldi\n",
    "        \n",
    "        h_size = int(n_coins)\n",
    "        a_size = self.n_actions\n",
    "        self.fc1 = nn.Linear(n_players+1, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "                \n",
    "        self.opt = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform = Categorical(0.5*torch.ones([1, 2]))\n",
    "uniform.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_players, n_coins, episode_len, n_actions=2, lr = 0.01, _gamma = 0.99):\n",
    "        \n",
    "        self.n_players = n_players\n",
    "        self.n_coins = n_coins\n",
    "        self.n_actions = n_actions # 0 non do soldi, 1 do soldi\n",
    "        self.episode_len = episode_len\n",
    "     \n",
    "        self.commnet = CommNet(self.n_coins)\n",
    "        self.actnet = ActNet(self.n_players, self.n_coins)\n",
    "        self.epsilon = 0.1\n",
    "        self._gamma = _gamma\n",
    "        \n",
    "        self.memory = Memory()\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.memory.reset()\n",
    "    \n",
    "    def act(self, coin, messages): \n",
    "\n",
    "        coin = torch.Tensor([coin])\n",
    "        \n",
    "        state = torch.cat((coin, messages), 0).to(device).unsqueeze(dim=0)\n",
    "        \n",
    "        probs_total = self.actnet.forward(state)\n",
    "        probs_total = F.softmax(probs_total, dim=1)\n",
    "        \n",
    "        m = Categorical(probs_total)\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            uniform = Categorical(0.5*torch.ones([1, 2]))\n",
    "            action = uniform.sample()\n",
    "        else:\n",
    "            action = m.sample()\n",
    "        \n",
    "        log_prob = m.log_prob(action)\n",
    "        \n",
    "        self.memorize(state, action, log_prob)\n",
    "\n",
    "        return action, log_prob, probs_total\n",
    "\n",
    "    def comm(self, coin):\n",
    "        \n",
    "        state = np.array(int(coin))\n",
    "\n",
    "        state = torch.from_numpy(state).unsqueeze(dim=0).to(device)\n",
    "        state = F.one_hot(state, num_classes=self.n_coins+1).float()\n",
    "        \n",
    "        probs_total = self.commnet.forward(state)\n",
    "        probs_total = F.softmax(probs_total, dim=1)\n",
    "        \n",
    "        m = Categorical(probs_total)\n",
    "\n",
    "        action = m.sample()\n",
    "\n",
    "        log_prob = m.log_prob(action)\n",
    "            \n",
    "        self.memorize_comm(state, action, log_prob)\n",
    "\n",
    "        return action, log_prob, probs_total\n",
    "    \n",
    "    def memorize(self, s, a, lp):\n",
    "        self.memory.push(s, a, lp)\n",
    "        \n",
    "    def memorize_comm(self, s, a, lp):\n",
    "        self.memory.push_comm(s, a, lp)\n",
    "    \n",
    "    def memorize_rewards(self, r):\n",
    "        self.memory.save_reward(r)\n",
    "    \n",
    "    def compute_G(self):\n",
    "        G = np.zeros((self.episode_len))\n",
    "        cumulative_rewards = 0\n",
    "        \n",
    "        for t in reversed(range(0, self.episode_len)):\n",
    "            cumulative_rewards = cumulative_rewards * self._gamma + self.memory.rewards[t]\n",
    "            G[t] = cumulative_rewards\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1]),\n",
       " tensor([-0.7689], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[0.5365, 0.4635]], grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_players = 3\n",
    "n_coins = 15\n",
    "messages = torch.Tensor((0,0,0))\n",
    "my_coins = 3\n",
    "ag = Agent(n_players = n_players, n_coins = n_coins, episode_len=1)\n",
    "ag.act(my_coins, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self, n_players, n_total_coins, threshold = 0., multiplier = 1.):\n",
    "        self.n_players = n_players\n",
    "        self.n_total_coins = n_total_coins\n",
    "        self.state = np.zeros((self.n_players,1))\n",
    "        self.threshold = threshold\n",
    "        self.multiplier = multiplier\n",
    "        self.rews_dict = {k: [] for k in range(self.n_players)}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.n_players)\n",
    "        self.rews_dict = {k: [] for k in range(self.n_players)}\n",
    "        \n",
    "    def equal_division(self):\n",
    "        self.coins = [int(self.n_total_coins/self.n_players) for i in range(self.n_players)]\n",
    "        return self.coins\n",
    "    \n",
    "    def update_rews_dict(self, rew, t):\n",
    "        for i in range(self.n_players):\n",
    "            self.rews_dict[i].append(rew[i])\n",
    "            \n",
    "    def sample(self):\n",
    "        left_coins = self.n_total_coins\n",
    "        self.coins = []\n",
    "        \n",
    "        for i in range(self.n_players):\n",
    "            if (i == self.n_players - 1):\n",
    "                self.coins.append(left_coins)\n",
    "            else:\n",
    "                val = int(np.random.choice(left_coins-1, 1))\n",
    "                self.coins.append(val)\n",
    "                left_coins = left_coins - val \n",
    "                \n",
    "        return self.coins\n",
    "\n",
    "    def step(self, coins, actions):\n",
    "        rewards = np.zeros(self.n_players)\n",
    "        \n",
    "        if (np.sum(actions)) < self.threshold:\n",
    "            return rewards\n",
    "        \n",
    "        amount = np.sum([coins[i] for i in range(self.n_players) if actions[i] == 1])\n",
    "            \n",
    "        for i in range(self.n_players):\n",
    "            if (actions[i] == 1.):\n",
    "                rewards[i] = amount/self.n_players*self.multiplier\n",
    "            else:\n",
    "                rewards[i] = amount/self.n_players*self.multiplier + coins[i]\n",
    "                \n",
    "            # trick to avoid log(0) \n",
    "            if rewards[i] <= 0:\n",
    "                rewards[i] = 1e-6\n",
    "\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class System():\n",
    "    \n",
    "    def __init__(self, n_players, n_coins, episode_len, epsilon=0.1, n_actions=2, lr=0.01, _gamma=0.99):\n",
    "        \n",
    "        self.n_players = n_players\n",
    "        self.n_coins = n_coins\n",
    "        self.n_actions = n_actions # 0 non do soldi, 1 do soldi\n",
    "        self.episode_len = episode_len\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.agents = {k: Agent(n_players, n_coins, episode_len=episode_len) for k in range(self.n_players)}\n",
    "\n",
    "    def reset(self):\n",
    "        for _, ag in self.agents.items(): ag.reset()\n",
    "            \n",
    "    def comm(self, coins):\n",
    "        mexs = torch.zeros(self.n_players)\n",
    "        log_probs_mexs = np.zeros(self.n_players)\n",
    "        \n",
    "        for i, agent in self.agents.items():\n",
    "            m, log_prob, _ = agent.comm(coins[i])\n",
    "            mexs[i] = m.detach().numpy()[0]\n",
    "            log_probs_mexs[i] = log_prob\n",
    "            \n",
    "        return mexs, log_probs_mexs\n",
    "            \n",
    "    def act(self, coins, mexs):\n",
    "        actions = np.zeros(self.n_players)\n",
    "        log_probs = np.zeros(self.n_players)\n",
    "        \n",
    "        for i, agent in self.agents.items():\n",
    "            a, log_prob, _ = agent.act(coins[i], mexs)\n",
    "            actions[i] = a.detach().numpy()[0]\n",
    "            log_probs[i] = log_prob\n",
    "            \n",
    "        return actions, log_probs\n",
    "    \n",
    "    def memorize_rewards(self, rews):\n",
    "        for idx, ag in self.agents.items(): ag.memorize_rewards(rews[idx])\n",
    "            \n",
    "    def reinforce(self):\n",
    "        self.losses = []\n",
    "        #print(\"\\nREINFORCE ACTIONS\")\n",
    "        \n",
    "        for agent_idx, agent in self.agents.items():\n",
    "\n",
    "            agent_G = agent.compute_G()\n",
    "            \n",
    "            #print(\"ag.G=\",agent.G)\n",
    "            loss = 0\n",
    "            for G, log_prob in zip(agent_G, agent.memory.log_probs):\n",
    "                loss -= log_prob*G\n",
    "            \n",
    "            self.losses.append(loss.detach().numpy()[0])\n",
    "            \n",
    "            agent.actnet.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            agent.actnet.opt.step()\n",
    "            \n",
    "    def reinforce_comm(self):\n",
    "        self.losses_comm = []\n",
    "        #print(\"\\nREINFORCE COMMUNICATION\")\n",
    "        \n",
    "        for agent_idx, agent in self.agents.items():\n",
    "\n",
    "            agent_G = agent.compute_G()\n",
    "            loss = 0\n",
    "            for G, log_prob in zip(agent_G, agent.memory.log_probs_mexs):\n",
    "                loss -= log_prob*G\n",
    "            \n",
    "            self.losses_comm.append(loss.detach().numpy()[0])\n",
    "        \n",
    "            agent.commnet.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            agent.commnet.opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_players_communication(n_players=5, n_coins=10, n_episodes=1, episode_len=10, epsilon=0.1, \\\n",
    "                              threshold=0., multiplier=1., gamma=0.9):\n",
    "    \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['episode', 't', 'loss0', 'loss1', 'loss2'])\n",
    "                      \n",
    "    env = Environment(n_players, n_coins, threshold, multiplier)\n",
    "    system = System(n_players, n_coins, episode_len, epsilon)\n",
    "    \n",
    "    history_rewards = {k: [] for k in range(n_players)}\n",
    "\n",
    "    line = 0\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        \n",
    "        env.reset()\n",
    "        system.reset()\n",
    "                \n",
    "        for t in range(0, episode_len):\n",
    "            \n",
    "            coins = env.equal_division()\n",
    "            \n",
    "            #print(\"\\ncoins=\", coins)\n",
    "\n",
    "            mex, log_prob_mex = system.comm(coins)\n",
    "            #print(\"mexs=\", mex)\n",
    "            \n",
    "            act, log_prob = system.act(coins, mex)\n",
    "            #print(\"acts=\", act)\n",
    "            \n",
    "            rews = env.step(coins, act)\n",
    "            #print(\"rews=\", rews)\n",
    "            \n",
    "            env.update_rews_dict(rews, t)\n",
    "            \n",
    "            system.memorize_rewards(rews)\n",
    "        \n",
    "        system.reinforce()\n",
    "        system.reinforce_comm()\n",
    "                  \n",
    "        df.loc[line] = [e, t] + system.losses\n",
    "        line += 1\n",
    "        \n",
    "        \n",
    "        if (e%100 == 0):\n",
    "            print(\"==> Episode=\", e)\n",
    "            print(system.losses)\n",
    "            \n",
    "\n",
    "    return df, history_rewards, system.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d60b09257b88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmultiplier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreinforce_players_communication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_players\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_players\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_coins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_coins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-c041acde0dc1>\u001b[0m in \u001b[0;36mreinforce_players_communication\u001b[0;34m(n_players, n_coins, n_episodes, episode_len, epsilon, threshold, multiplier, gamma)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m#print(\"mexs=\", mex)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;31m#print(\"acts=\", act)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-1642a4355582>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, coins, mexs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmexs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-aaac96373163>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, coin, messages)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprobs_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "n_episodes = 1000\n",
    "episode_len = 5\n",
    "\n",
    "n_players = 3\n",
    "n_coins = 9\n",
    "\n",
    "threshold = 0.\n",
    "multiplier = 2.\n",
    "\n",
    "df,hr, agents = reinforce_players_communication(n_players=n_players, n_coins=n_coins, n_episodes=n_episodes, episode_len=episode_len, threshold = threshold, multiplier=multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1, n_players, figsize=(16, 4))\n",
    "\n",
    "for i in range(n_players):\n",
    "    ax[i].axhline(y=n_coins*multiplier/n_players, color='r', linestyle='-')\n",
    "    ax[i].plot(np.linspace(0, n_episodes*episode_len, n_episodes*episode_len), agents[i].memory.all_rewards, label='reward agent'+str(i))\n",
    "    ax[i].legend()\n",
    "    ax[i].grid()\n",
    "    ax[i].set_title(\"Reward agent \"+str(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1, n_players, figsize=(16, 4))\n",
    "\n",
    "for i in range(n_players):\n",
    "    ax[i].plot(np.linspace(0, n_episodes, n_episodes), df['loss'+str(i)], label='loss agent'+str(i))\n",
    "    ax[i].legend()\n",
    "    ax[i].grid()\n",
    "    ax[i].set_title(\"Loss agent \"+str(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coins = 3\n",
    "mex = torch.Tensor([0., 0., 0.])\n",
    "\n",
    "figure, ax = plt.subplots(2, 3, figsize=(16, 4))\n",
    "\n",
    "for i in range(n_players):\n",
    "    mex[i], log_prob_mex, probs_mex = agents[i].comm(coins)\n",
    "    probbs_mex = probs_mex.detach().numpy()\n",
    "    \n",
    "    sns.heatmap(probbs_mex, annot=True, linewidth=.5,ax=ax[0,i])\n",
    "    \n",
    "    ax[0,i].set_title(\"Probs messages agent \"+str(i))\n",
    "    \n",
    "print(\"mexs=\", mex)\n",
    "\n",
    "for i in range(n_players):\n",
    "    a, log_prob, probs = agents[i].act(coins, mex)\n",
    "    probbs = probs.detach().numpy()\n",
    "    \n",
    "    sns.heatmap(probbs, annot=True, linewidth=.5,ax=ax[1,i])\n",
    "    \n",
    "    ax[1,i].set_title(\"Probs actions agent \"+str(i))\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
