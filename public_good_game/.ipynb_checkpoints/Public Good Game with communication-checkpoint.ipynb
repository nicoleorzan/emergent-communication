{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import ceil, floor\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import Normal\n",
    "from scipy.stats import rv_discrete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.all_rewards = []\n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.mexs = []\n",
    "        self.log_probs_mexs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def push(self, s, a, lp):\n",
    "        self.states.append(s)        \n",
    "        self.actions.append(a)        \n",
    "        self.log_probs.append(lp)\n",
    "        \n",
    "    def push_comm(self, s, a, lp):\n",
    "        self.states.append(s)        \n",
    "        self.mexs.append(a)        \n",
    "        self.log_probs_mexs.append(lp)\n",
    "        \n",
    "    def save_reward(self, r):\n",
    "        self.rewards.append(r)        \n",
    "        self.all_rewards.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CommNet(nn.Module):\n",
    "    def __init__(self, n_coins, lr=0.01):\n",
    "        super(CommNet, self).__init__()\n",
    "        \n",
    "        self.n_coins = n_coins\n",
    "        self.lr = lr\n",
    "        self.n_actions = 2 # 0 non do soldi, 1 do soldi\n",
    "        \n",
    "        h_size = int(n_coins)\n",
    "        a_size = self.n_actions\n",
    "        self.fc1 = nn.Linear(n_coins+1, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "        \n",
    "        self.opt = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x): #entrata: n coins che ho\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        n = out.size()\n",
    "        mask = torch.zeros(n).to(device)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentNet(nn.Module):\n",
    "    def __init__(self, n_players, n_coins, episode_len, n_actions=2, lr = 0.01, _gamma = 0.99):\n",
    "        super(AgentNet, self).__init__()\n",
    "        \n",
    "        self.n_players = n_players\n",
    "        self.n_coins = n_coins\n",
    "        self.n_actions = n_actions # 0 non do soldi, 1 do soldi\n",
    "        self.episode_len = episode_len\n",
    "        \n",
    "        h_size = int(n_coins)\n",
    "        a_size = self.n_actions\n",
    "        self.fc1 = nn.Linear(n_players+1, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "        self.commnet = CommNet(self.n_coins)\n",
    "        \n",
    "        self._gamma = _gamma\n",
    "        \n",
    "        self.memory = Memory()\n",
    "        self.opt = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.memory.reset()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def act(self, coin, messages): \n",
    "\n",
    "        coin = torch.Tensor([coin])\n",
    "        #coins = np.array(int(coins))\n",
    "        \n",
    "        state = torch.cat((coin, messages), 0).to(device).unsqueeze(dim=0)\n",
    "        \n",
    "        #state = torch.from_numpy(state).unsqueeze(dim=0).to(device)\n",
    "        #print(\"state=\", state)\n",
    "\n",
    "        probs_total = self.forward(state).cpu()\n",
    "  \n",
    "        probs_total = F.softmax(probs_total, dim=1)\n",
    "        \n",
    "        m = Categorical(probs_total)\n",
    "\n",
    "        action = m.sample()\n",
    "        \n",
    "        log_prob = m.log_prob(action)\n",
    "        \n",
    "        self.memorize(state, action, log_prob)\n",
    "\n",
    "        return action, log_prob, probs_total\n",
    "\n",
    "    def comm(self, coin):\n",
    "        \n",
    "        state = np.array(int(coin))\n",
    "\n",
    "        state = torch.from_numpy(state).unsqueeze(dim=0).to(device)\n",
    "        state = F.one_hot(state, num_classes=self.n_coins+1).float()\n",
    "        \n",
    "        probs_total = self.commnet.forward(state).cpu()\n",
    "        probs_total = F.softmax(probs_total, dim=1)\n",
    "        \n",
    "        m = Categorical(probs_total)\n",
    "\n",
    "        action = m.sample()\n",
    "\n",
    "        log_prob = m.log_prob(action)\n",
    "            \n",
    "        self.memorize_comm(state, action, log_prob)\n",
    "\n",
    "        return action, log_prob, probs_total\n",
    "    \n",
    "    def memorize(self, s, a, lp):\n",
    "        self.memory.push(s, a, lp)\n",
    "        \n",
    "    def memorize_comm(self, s, a, lp):\n",
    "        self.memory.push_comm(s, a, lp)\n",
    "    \n",
    "    def memorize_rewards(self, r):\n",
    "        self.memory.save_reward(r)\n",
    "    \n",
    "    def compute_G(self):\n",
    "        self.G = np.zeros((self.episode_len))\n",
    "        cumulative_rewards = 0\n",
    "        \n",
    "        for t in reversed(range(0, self.episode_len)):\n",
    "            cumulative_rewards = cumulative_rewards * self._gamma + self.memory.rewards[t]\n",
    "            self.G[t] = cumulative_rewards\n",
    "        return self.G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1]),\n",
       " tensor([-0.8742], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[0.5828, 0.4172]], grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_players = 3\n",
    "n_coins = 15\n",
    "messages = torch.Tensor((0,0,0))\n",
    "my_coins = torch.Tensor([3])\n",
    "ag = AgentNet(n_players = n_players, n_coins = n_coins, episode_len=1)\n",
    "ag.act(my_coins, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self, n_players, n_total_coins, threshold = 0., multiplier = 1.):\n",
    "        self.n_players = n_players\n",
    "        self.n_total_coins = n_total_coins\n",
    "        self.state = np.zeros((self.n_players,1))\n",
    "        self.threshold = threshold\n",
    "        self.multiplier = multiplier\n",
    "        self.rews_dict = {k: [] for k in range(self.n_players)}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.n_players)\n",
    "        self.rews_dict = {k: [] for k in range(self.n_players)}\n",
    "        \n",
    "    def equal_division(self):\n",
    "        self.coins = [int(self.n_total_coins/self.n_players) for i in range(self.n_players)]\n",
    "        return self.coins\n",
    "    \n",
    "    def update_rews_dict(self, rew, t):\n",
    "        for i in range(self.n_players):\n",
    "            self.rews_dict[i].append(rew[i])\n",
    "            \n",
    "    def sample(self):\n",
    "        left_coins = self.n_total_coins\n",
    "        self.coins = []\n",
    "        \n",
    "        for i in range(self.n_players):\n",
    "            if (i == self.n_players - 1):\n",
    "                self.coins.append(left_coins)\n",
    "            else:\n",
    "                val = int(np.random.choice(left_coins-1, 1))\n",
    "                self.coins.append(val)\n",
    "                left_coins = left_coins - val \n",
    "                \n",
    "        return self.coins\n",
    "\n",
    "    def step(self, coins, actions):\n",
    "        rewards = np.zeros(self.n_players)\n",
    "        \n",
    "        if (np.sum(actions)) < self.threshold:\n",
    "            return rewards\n",
    "        \n",
    "        amount = np.sum([coins[i] for i in range(self.n_players) if actions[i] == 1])\n",
    "            \n",
    "        for i in range(self.n_players):\n",
    "            if (actions[i] == 1.):\n",
    "                rewards[i] = amount/self.n_players*self.multiplier\n",
    "            else:\n",
    "                rewards[i] = amount/self.n_players*self.multiplier + coins[i]\n",
    "                \n",
    "            # trick to avoid log(0) \n",
    "            if rewards[i] <= 0:\n",
    "                rewards[i] = 1e-6\n",
    "\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class System():\n",
    "    \n",
    "    def __init__(self, n_players, n_coins, episode_len, epsilon=0.1, n_actions=2, lr=0.01, _gamma=0.99):\n",
    "        \n",
    "        self.n_players = n_players\n",
    "        self.n_coins = n_coins\n",
    "        self.n_actions = n_actions # 0 non do soldi, 1 do soldi\n",
    "        self.episode_len = episode_len\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.agents = {k: AgentNet(n_players, n_coins, episode_len=episode_len) for k in range(self.n_players)}\n",
    "\n",
    "    def reset(self):\n",
    "        for _, ag in self.agents.items(): ag.reset()\n",
    "            \n",
    "    def comm(self, coins):\n",
    "        mexs = torch.empty(self.n_players)\n",
    "        log_probs_mexs = np.zeros(self.n_players)\n",
    "        \n",
    "        for i, agent in self.agents.items():\n",
    "            m, log_prob, _ = agent.comm(coins[i])\n",
    "            mexs[i] = m.detach().numpy()[0]\n",
    "            log_probs_mexs[i] = log_prob\n",
    "            \n",
    "        return mexs, log_probs_mexs\n",
    "            \n",
    "    def act(self, coins, mexs):\n",
    "        actions = np.zeros(self.n_players)\n",
    "        log_probs = np.zeros(self.n_players)\n",
    "        \n",
    "        for i, agent in self.agents.items():\n",
    "            a, log_prob, _ = agent.act(coins[i], mexs)\n",
    "            actions[i] = a.detach().numpy()[0]\n",
    "            log_probs[i] = log_prob\n",
    "            \n",
    "        return actions, log_probs\n",
    "    \n",
    "    def memorize_rewards(self, rews):\n",
    "        for idx, ag in self.agents.items(): ag.memorize_rewards(rews[idx])\n",
    "            \n",
    "    def reinforce(self):\n",
    "        self.losses = []\n",
    "        for agent_idx, agent in self.agents.items():\n",
    "            #print(\"\\nagent=\", agent_idx)\n",
    "            agent.compute_G()\n",
    "            #print(\"ag.G=\",agent.G)\n",
    "            loss = 0\n",
    "            for G, log_prob in zip(agent.G, agent.memory.log_probs):\n",
    "                loss -= log_prob*G\n",
    "                print(log_prob._version)\n",
    "            print(\"loss=\", loss, loss._version)\n",
    "            \n",
    "            self.losses.append(loss.detach().numpy()[0])\n",
    "            \n",
    "            agent.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            agent.opt.step()\n",
    "            \n",
    "    def reinforce_comm(self):\n",
    "        self.losses_comm = []\n",
    "        for agent_idx, agent in self.agents.items():\n",
    "            #print(\"\\nagent=\", agent_idx)\n",
    "            #agent.compute_G()\n",
    "            loss = 0\n",
    "            for G, log_prob in zip(agent.G, agent.memory.log_probs_mexs):\n",
    "                loss -= log_prob*G\n",
    "            print(\"comm loss=\", loss, loss._version)\n",
    "            \n",
    "            self.losses_comm.append(loss.detach().numpy()[0])\n",
    "            agent.commnet.opt.zero_grad()\n",
    "            print(agent.commnet.parameters()[0]._version)\n",
    "            loss.backward()\n",
    "            agent.commnet.opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_players_communication(n_players=5, n_coins=10, n_episodes=1, episode_len=10, epsilon=0.1, \\\n",
    "                              threshold=0., multiplier=1., gamma=0.9):\n",
    "    \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['episode', 't', 'loss0', 'loss1', 'loss2'])\n",
    "                      \n",
    "    env = Environment(n_players, n_coins, threshold, multiplier)\n",
    "    system = System(n_players, n_coins, episode_len, epsilon)\n",
    "    \n",
    "    history_rewards = {k: [] for k in range(n_players)}\n",
    "\n",
    "    line = 0\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        #print(\"\\nEpisode=\", e)\n",
    "        \n",
    "        env.reset()\n",
    "        system.reset()\n",
    "        \n",
    "        if ( e == int(n_episodes/2) ):\n",
    "            for agent_idx, ag in system.agents.items():\n",
    "                ag.epsilon = 0.1\n",
    "        if (e == int(n_episodes - 50)):\n",
    "            ag.epsilon = 0.0001\n",
    "                \n",
    "        for t in range(0, episode_len):\n",
    "            \n",
    "            coins = env.equal_division()\n",
    "            \n",
    "            print(\"\\ncoins=\", coins)\n",
    "\n",
    "            mex, log_prob_mex = system.comm(coins)\n",
    "            print(\"mexs=\", mex)\n",
    "            \n",
    "            act, log_prob = system.act(coins, mex)\n",
    "            print(\"acts=\", act)\n",
    "            \n",
    "            rews = env.step(coins, act)\n",
    "            print(\"rews=\", rews)\n",
    "            \n",
    "            env.update_rews_dict(rews, t)\n",
    "            \n",
    "            system.memorize_rewards(rews)\n",
    "            \n",
    "        #print(\"act=\", act)\n",
    "        #print(\"rews=\", rews)\n",
    "        \n",
    "        system.reinforce()\n",
    "        system.reinforce_comm()\n",
    "                  \n",
    "        df.loc[line] = [e, t] + system.losses\n",
    "        line += 1\n",
    "\n",
    "        \n",
    "        if (e%100 == 0):\n",
    "            print(\"==> Episode=\", e)\n",
    "            print(system.losses)\n",
    "            #print(\"epsilon=\", system.agents[0].epsilon)\n",
    "            #print(\"act=\", act)\n",
    "            #print(\"rews=\", rews)\n",
    "            \n",
    "    return df, history_rewards, system.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "coins= [3, 3, 3]\n",
      "mexs= tensor([0., 0., 1.])\n",
      "acts= [1. 1. 0.]\n",
      "rews= [ 9.  9. 12.]\n",
      "0\n",
      "loss= tensor([5.5930], grad_fn=<RsubBackward1>) 0\n",
      "0\n",
      "loss= tensor([7.4820], grad_fn=<RsubBackward1>) 0\n",
      "0\n",
      "loss= tensor([11.7344], grad_fn=<RsubBackward1>) 0\n",
      "comm loss= tensor([5.6571], grad_fn=<RsubBackward1>) 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Adam' object has no attribute '_version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-1cbf73d23ab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmultiplier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_players\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreinforce_players_communication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_players\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_players\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_coins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_coins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-8fabe26b27e3>\u001b[0m in \u001b[0;36mreinforce_players_communication\u001b[0;34m(n_players, n_coins, n_episodes, episode_len, epsilon, threshold, multiplier, gamma)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinforce_comm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-f3cb64c11535>\u001b[0m in \u001b[0;36mreinforce_comm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses_comm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Adam' object has no attribute '_version'"
     ]
    }
   ],
   "source": [
    "n_episodes = 2\n",
    "episode_len = 1\n",
    "\n",
    "n_players = 3\n",
    "n_coins = 10\n",
    "\n",
    "threshold = 0.\n",
    "multiplier = float(n_players)/2.+3.\n",
    "\n",
    "history, agents, hr = reinforce_players_communication(n_players=n_players, n_coins=n_coins, n_episodes=n_episodes, episode_len=episode_len, threshold = threshold, multiplier=multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1, n_players, figsize=(16, 4))\n",
    "\n",
    "for i in range(n_players):\n",
    "    ax[i].plot(np.linspace(0, n_episodes*episode_len, n_episodes*episode_len), agents[i].memory.all_rewards, label='reward agent'+str(i))\n",
    "    ax[i].legend()\n",
    "    ax[i].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1, n_players, figsize=(16, 4))\n",
    "\n",
    "for i in range(n_players):\n",
    "    ax[i].plot(np.linspace(0, n_episodes, n_episodes), df['loss'+str(i)], label='loss agent'+str(i))\n",
    "    ax[i].legend()\n",
    "    ax[i].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probbs = np.zeros((n_coins, 2))\n",
    "\n",
    "for c in range(n_coins):\n",
    "    a, log_prob, probs = agents[0].act(np.array(c))\n",
    "    probbs[c] = probs.detach().numpy()\n",
    "    \n",
    "ax = sns.heatmap(probbs, annot=True, linewidth=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probbs = np.zeros((n_coins, 2))\n",
    "\n",
    "for c in range(n_coins):\n",
    "    a, log_prob, probs = agents[0].act(np.array(c))\n",
    "    probbs[c] = probs.detach().numpy()\n",
    "    \n",
    "ax = sns.heatmap(probbs, annot=True, linewidth=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probbs = np.zeros((n_coins, 2))\n",
    "\n",
    "for c in range(n_coins):\n",
    "    a, log_prob, probs = agents[0].act(np.array(c))\n",
    "    probbs[c] = probs.detach().numpy()\n",
    "    \n",
    "ax = sns.heatmap(probbs, annot=True, linewidth=.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
